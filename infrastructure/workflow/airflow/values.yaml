# 1. 실행 방식: KubernetesExecutor
# 각 Task가 별도의 Pod로 생성되어 실행된 후 종료됩니다. 리소스 효율이 높습니다.
executor: KubernetesExecutor

# 2. 이미지 설정
images:
  airflow:
    repository: apache/airflow
    tag: 2.10.2 # 2025-11-10 기준 Stable 버전 사용
    pullPolicy: IfNotPresent

# 3. 환경 변수 및 설정
config:
  core:
    dags_folder: /opt/airflow/dags/repo/dags # Git-Sync 경로
    load_examples: "False" # 예제 DAG 비활성화
  logging:
    remote_logging: "False" # 로컬 PVC 로깅 사용

# 4. 데이터베이스 연결 (Docker Host의 Postgres 사용)
data:
  metadataSecretName: airflow-secrets # 위에서 만든 Secret 참조
  metadataConnection:
    user: airflow_user
    pass: <secret> # Secret에서 자동 주입
    protocol: postgresql
    # 중요: 이전 단계에서 생성할 External Service(Headless) 주소
    host: postgres-external.default.svc.cluster.local
    port: 15432
    db: airflow_db

# 5. Redis 연결 (Docker Host의 Redis 사용 - KubernetesExecutor는 필수는 아니나 Celery 전환 대비)
redis:
  enabled: false # KubernetesExecutor는 Redis가 필수 아님. CeleryExecutor 사용 시 true 및 외부 연결 설정 필요.

# 6. 내부 구성요소 비활성화 (외부 서비스 사용하므로)
postgresql:
  enabled: false
statsd:
  enabled: false # Prometheus/Alloy 사용 예정

# 7. GitSync (DAG 동기화)
dags:
  gitSync:
    enabled: true
    repo: https://github.com/buenhyden/hy-home.k8s.git
    branch: main
    rev: HEAD
    depth: 1
    subPath: "dags" # 리포지토리 내 DAGs 폴더 경로
    wait: 60 # 동기화 대기 시간 (초)
    containerName: git-sync
    resources:
      requests:
        cpu: 50m
        memory: 64Mi

# 8. 로그 영속성 (PVC)
logs:
  persistence:
    enabled: true
    storageClassName: standard # MinIO/LocalPath
    size: 10Gi

# 9. 서비스 어카운트 (Worker Pod 생성을 위해 권한 필요)
serviceAccount:
  create: true
  name: airflow-worker

# 10. 마이그레이션 작업
migrateDatabaseJob:
  enabled: true
  jobAnnotations:
    "argocd.argoproj.io/hook": Sync # ArgoCD Sync 시 자동 실행
