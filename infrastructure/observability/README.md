# Observability Infrastructure

Complete observability stack for metrics, logs, and distributed tracing.

## Components

- **kube-prometheus-stack**: Prometheus, Grafana, Alertmanager
- **loki**: Log aggregation
- **tempo**: Distributed tracing
- **alloy**: Unified telemetry collector (replaces promtail and tempo agent)

## Architecture

```
Applications → Alloy → Prometheus (metrics)
                   → Loki (logs)
                   → Tempo (traces)
                   → Grafana (visualization)
```

## Namespace

All observability components are deployed in the `observability` namespace.

## kube-prometheus-stack

### Purpose

Complete Prometheus monitoring stack with Grafana visualization.

### Version

- **Chart**: prometheus-community/kube-prometheus-stack v66.7.1
- **Prometheus**: v2.51.0
- **Grafana**: v10.4.1

### Components

1. **Prometheus** - Metrics collection and storage
2. **Grafana** - Visualization dashboards
3. **Alertmanager** - Alert routing and management
4. **Prometheus Operator** - Manages Prometheus CRDs
5. **Node Exporter** - Node-level metrics
6. **Kube State Metrics** - Kubernetes object metrics

### Key Configuration

**Prometheus**:

```yaml
prometheus:
  prometheusSpec:
    retention: 10d
    storageSpec:
      volumeClaimTemplate:
        spec:
          resources:
            requests:
              storage: 20Gi
```

**Grafana**:

```yaml
grafana:
  adminPassword: <auto-generated>
  persistence:
    enabled: true
    size: 5Gi
```

### Access Grafana

```bash
# Port-forward to Grafana
kubectl port-forward -n observability svc/kube-prometheus-stack-grafana 3000:80

# Visit http://localhost:3000
```

**Default Credentials**:

- Username: `admin`
- Password: Retrieved via:

  ```bash
  kubectl get secret -n observability kube-prometheus-stack-grafana \
    -o jsonpath="{.data.admin-password}" | base64 -d
  ```

### Pre-installed Dashboards

Grafana comes with pre-installed dashboards:

- Kubernetes Cluster Monitoring
- Node Exporter Full
- Prometheus Stats
- Istio Mesh Dashboard
- Istio Service Dashboard

### Custom Metrics

**Add ServiceMonitor**:

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-app
  namespace: default
spec:
  selector:
    matchLabels:
      app: my-app
  endpoints:
    - port: metrics
      interval: 30s
```

## Loki

### Purpose

Log aggregation system designed to work with Grafana.

### Version

- **Chart**: grafana/loki v6.18.0
- **Loki**: v3.0.0

### Configuration

**Mode**: Simple Scalable (single binary)

**Retention**: 7 days

**Storage**:

```yaml
loki:
  storage:
    type: filesystem
  schemaConfig:
    configs:
      - from: "2024-01-01"
        store: tsdb
        object_store: filesystem
        schema: v13
```

### Querying Logs

**Via Grafana**:

1. Open Grafana
2. Go to Explore
3. Select Loki data source
4. Use LogQL queries:

   ```
   {namespace="default", app="my-app"}
   {namespace="default"} |= "error"
   ```

**Via logcli**:

```bash
# Install logcli
# Query logs
logcli query '{namespace="default"}' --addr=http://<loki-url>:3100
```

## Tempo

### Purpose

Distributed tracing backend compatible with Jaeger, Zipkin, and OpenTelemetry.

### Version

- **Chart**: grafana/tempo v1.12.0
- **Tempo**: v2.6.1

### Configuration

**Mode**: Monolithic (single binary)

**Storage**: Filesystem

**Ingestion**:

- OpenTelemetry (OTLP): Port 4317 (gRPC), 4318 (HTTP)
- Jaeger (gRPC): Port 14250
- Zipkin: Port 9411

### Sending Traces

**OpenTelemetry (Recommended)**:

```yaml
# Configure your app to send traces to:
OTEL_EXPORTER_OTLP_ENDPOINT: http://tempo.observability.svc:4317
```

**Python Example**:

```python
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Setup
provider = TracerProvider()
processor = BatchSpanProcessor(
    OTLPSpanExporter(endpoint="http://tempo.observability.svc:4317")
)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)
```

### Viewing Traces

**Via Grafana**:

1. Open Grafana Explore
2. Select Tempo data source
3. Search by TraceID or use query filters

## Alloy

### Purpose

Unified telemetry collector replacing multiple agents (promtail, tempo agent, etc.).

### Version

- **Chart**: grafana/alloy v0.9.0

### Capabilities

- **Metrics**: Scrapes Prometheus metrics
- **Logs**: Collects logs and sends to Loki
- **Traces**: Receives traces and forwards to Tempo

### Configuration

**Logs Pipeline**:

```yaml
local.file_match "logs" {
  path_targets = [{"__path__" = "/var/log/pods/*/*.log"}]
}

loki.source.kubernetes "pods" {
  targets    = local.file_match.logs.targets
  forward_to = [loki.write.default.receiver]
}

loki.write "default" {
  endpoint {
    url = "http://loki.observability.svc:3100/loki/api/v1/push"
  }
}
```

## Troubleshooting

### Prometheus Not Scraping

**Check ServiceMonitor**:

```bash
kubectl get servicemonitor -n default
kubectl describe servicemonitor my-app -n default
```

**Check Prometheus targets**:

1. Port-forward Prometheus:

   ```bash
   kubectl port-forward -n observability svc/kube-prometheus-stack-prometheus 9090:9090
   ```

2. Visit <http://localhost:9090/targets>
3. Verify target is UP

### Grafana No Data

**Check data sources**:

1. Open Grafana Settings → Data Sources
2. Test connection to Prometheus, Loki, Tempo

**Check dashboards**:

- Ensure time range is correct
- Verify metrics/logs exist for selected filters

### Loki Logs Not Showing

**Check Alloy**:

```bash
kubectl logs -n observability -l app.kubernetes.io/name=alloy
```

**Verify log ingestion**:

```bash
# Query Loki directly
kubectl port-forward -n observability svc/loki 3100:3100
curl -G -s "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query={namespace="default"}'
```

### Tempo No Traces

**Check Tempo ingestion**:

```bash
kubectl logs -n observability -l app.kubernetes.io/name=tempo
```

**Verify endpoint**:

```bash
kubectl get svc -n observability tempo
# Ensure ports 4317, 4318 are exposed
```

## Alerts

### Creating PrometheusRule

```yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: my-app-alerts
  namespace: default
spec:
  groups:
    - name: my-app
      rules:
        - alert: HighErrorRate
          expr: |
            rate(http_requests_total{status=~"5.."}[5m]) > 0.05
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High error rate detected"
```

### Viewing Alerts

```bash
# Port-forward Alertmanager
kubectl port-forward -n observability svc/kube-prometheus-stack-alertmanager 9093:9093

# Visit http://localhost:9093
```

## Dashboards

### Import Custom Dashboard

1. Open Grafana → Dashboards → Import
2. Upload JSON or enter dashboard ID from grafana.com
3. Select Prometheus data source

### Export Dashboard

1. Open dashboard
2. Click Share → Export → Save to file
3. Commit JSON to Git

## Retention Policies

- **Prometheus**: 10 days
- **Loki**: 7 days
- **Tempo**: 7 days (configurable)

Adjust retention based on your needs and storage capacity.

## Performance Tuning

### Prometheus

**Reduce cardinality**:

- Avoid high-cardinality labels (user IDs, IPs)
- Use `metric_relabel_configs` to drop unnecessary labels

**Increase scrape interval**:

```yaml
global:
  scrape_interval: 60s  # Default is 30s
```

### Loki

**Limit log volume**:

- Filter logs at source (don't send debug logs)
- Use log sampling for high-volume services

### Tempo

**Sampling**:

```yaml
# Sample 10% of traces
sampling:
  probabilistic:
    sampling_percentage: 10
```

## Best Practices

1. **Use consistent labels**: Apply`app`, `environment`, `version` labels
2. **Monitor your monitoring**: Create alerts for Prometheus/Loki/Tempo health
3. **Dashboards as code**: Store Grafana dashboards in Git
4. **Use PrometheusRule CRDs**: Define alerts declaratively
5. **Test alerts**: Trigger test alerts to validate routing

## Accessing Services

```bash
# Prometheus
kubectl port-forward -n observability svc/kube-prometheus-stack-prometheus 9090:9090
# http://localhost:9090

# Grafana
kubectl port-forward -n observability svc/kube-prometheus-stack-grafana 3000:80
# http://localhost:3000

# Alertmanager
kubectl port-forward -n observability svc/kube-prometheus-stack-alertmanager 9093:9093
# http://localhost:9093

# Loki
kubectl port-forward -n observability svc/loki 3100:3100
# http://localhost:3100

# Tempo
kubectl port-forward -n observability svc/tempo 3200:3200
# http://localhost:3200
```

## References

- [Prometheus Documentation](https://prometheus.io/docs/)
- [Grafana Documentation](https://grafana.com/docs/)
- [Loki Documentation](https://grafana.com/docs/loki/latest/)
- [Tempo Documentation](https://grafana.com/docs/tempo/latest/)
- [Alloy Documentation](https://grafana.com/docs/alloy/latest/)
- [PromQL Basics](https://prometheus.io/docs/prometheus/latest/querying/basics/)
- [LogQL Guide](https://grafana.com/docs/loki/latest/query/)
